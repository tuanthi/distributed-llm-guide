$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json
name: distributed-llm-code-endpoint
description: "Production endpoint for distributed LLM code generation models"
auth_mode: key
public_network_access_enabled: true

tags:
  model_type: code_generation
  environment: production
  version: "1.0"

---
$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: distributed-llm-code-deployment
endpoint_name: distributed-llm-code-endpoint

# Model reference
model: azureml:distributed-llm-code-model:1

# Instance configuration
instance_type: Standard_DS3_v2
instance_count: 3

# Environment
environment: azureml:distributed-llm-serving-env:1

# Scaling configuration
scale_settings:
  type: target_utilization
  min_instances: 1
  max_instances: 10
  polling_interval: 30
  target_utilization_percentage: 70
  scale_up_cooldown: 300    # 5 minutes
  scale_down_cooldown: 600  # 10 minutes

# Request settings
request_settings:
  request_timeout_ms: 90000
  max_concurrent_requests_per_instance: 8
  max_queue_wait_ms: 60000

# Probes
liveness_probe:
  initial_delay: 10
  period: 30
  timeout: 10
  success_threshold: 1
  failure_threshold: 3

readiness_probe:
  initial_delay: 10
  period: 10
  timeout: 10
  success_threshold: 1
  failure_threshold: 30

# Resource requirements
resource_requirements:
  cpu: "2.0"
  memory: "8Gi"

# Code configuration
code_configuration:
  code: ../deployment
  scoring_script: score.py

# Environment variables
environment_variables:
  MODEL_NAME: "distributed-llm-code-model"
  DEVICE: "cpu"
  MAX_BATCH_SIZE: "8"
  OPTIMIZATION_LEVEL: "O2"

tags:
  deployment_type: production
  model_version: "1.0"
  created_by: azure_ml