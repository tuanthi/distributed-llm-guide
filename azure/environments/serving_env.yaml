$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json
name: distributed-llm-serving-env
version: "1.1"
description: "Optimized environment for distributed LLM model serving with TensorRT and ONNX"

# Optimized base image for inference
image: mcr.microsoft.com/azureml/curated/minimal-ubuntu20.04-py38-cpu-inference:latest

# Minimal conda environment for serving
conda_file: |
  name: distributed-llm-serving
  channels:
    - conda-forge
    - pytorch
  dependencies:
    - python=3.9
    - pip=23.0
    
    # Core inference frameworks
    - pytorch::pytorch=2.1.0
    - pytorch::torchvision=0.16.0
    - pytorch-cpu=2.1.0  # CPU-only for cost optimization
    
    # Essential packages
    - numpy=1.24.0
    - pillow=10.0.0
    
    # Web serving
    - uvicorn=0.24.0
    
    # Pip dependencies
    - pip:
      # HuggingFace (minimal)
      - transformers==4.36.0
      - tokenizers==0.15.0
      
      # Serving framework
      - fastapi==0.104.0
      - pydantic==2.5.0
      
      # Model optimization
      - onnxruntime==1.16.0
      - optimum==1.15.0
      
      # PEFT support
      - peft==0.7.0
      
      # Utilities
      - safetensors==0.4.0
      - einops==0.7.0
      
      # Monitoring
      - prometheus-client==0.19.0
      - psutil==5.9.0
      
      # Azure integration
      - azure-storage-blob==12.19.0
      
      # Async support
      - aiofiles==23.2.0
      - httpx==0.25.0

# Docker build context
build:
  dockerfile: |
    FROM mcr.microsoft.com/azureml/curated/minimal-ubuntu20.04-py38-cpu-inference:latest
    
    # Install system dependencies
    RUN apt-get update && apt-get install -y \
        curl \
        wget \
        unzip \
        && rm -rf /var/lib/apt/lists/*
    
    # Set working directory
    WORKDIR /app
    
    # Copy requirements and install Python packages
    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt
    
    # Copy application code
    COPY src/ ./src/
    COPY deployment/ ./deployment/
    
    # Set environment variables
    ENV PYTHONPATH="/app:${PYTHONPATH}"
    ENV MODEL_PATH="/app/models"
    ENV DEVICE="cpu"
    
    # Expose port
    EXPOSE 8000
    
    # Health check
    HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
      CMD curl -f http://localhost:8000/health || exit 1
    
    # Run the application
    CMD ["python", "-m", "uvicorn", "deployment.main:app", "--host", "0.0.0.0", "--port", "8000"]

tags:
  purpose: serving
  optimization: cpu
  use_case: inference